{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import bigrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler \n",
    "\n",
    "\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tpot import TPOTClassifier\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "\n",
    "# load spacy language model and save old tokenizer\n",
    "en_nlp = spacy.load('en')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "# replace the tokenizer with the preceding regexp\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
    "    regexp.findall(string))\n",
    "\n",
    "# create a custom tokenizer using the SpaCy document processing pipeline\n",
    "# (now using our own tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# caravan_A.csv contains a sample of data from the UDBC Zoopla dataset that we are going to use our classifier to \n",
    "# predict whether its a caravan or not.\n",
    "\n",
    "\n",
    "\n",
    "#.values.astype('U')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document, entity=False, parse=False)\n",
    "    return [token.lemma_ for token in doc_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_str(text, pattern):\n",
    "\n",
    "    try:\n",
    "\n",
    "        if filter(text.__contains__, pattern):\n",
    "            return (text.index(pattern))\n",
    "    except ValueError:\n",
    "        return -5000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_str2(s, char):\n",
    "    index = 0\n",
    "\n",
    "    if char in s:\n",
    "        c = char[0]\n",
    "        for ch in s:\n",
    "            if ch == c:\n",
    "                if s[index:index+len(char)] == char:\n",
    "                    return index\n",
    "\n",
    "            index += 1\n",
    "\n",
    "    return -5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.78 µs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.39 µs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.63 µs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test methods to find the faster one. even a few milliseconds less matter a lot for a function to be used on 11M rows.\n",
    "\n",
    "%time (find_str(\"not only a development but also kind of a park home.\",\"gorrila\"))\n",
    "%time (find_str(\"not only python but scala and spark and monkeys\",\"python\"))\n",
    "\n",
    "%time (find_str2(\"not only a development but also kind of a park home.\",\"gorrila\"))\n",
    "%time (find_str2(\"not only python but scala and spark and monkeys\",\"python\"))\n",
    "\n",
    "\n",
    "#based on speed of finding a word and not finding the work on a dataframe row 1st function is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_LR_clf = Pipeline([\n",
    "\n",
    "\n",
    "    ('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "                                   ngram_range=(1, 3),\n",
    "                                   tokenizer=custom_tokenizer,         \n",
    "                                   max_features=10000)),\n",
    "\n",
    "   \n",
    "    ('transformer', TfidfTransformer()),  # tfidf weighting on Document/Text Matrix\n",
    "\n",
    "    #('best', TruncatedSVD(n_components=200)),  # if we need dimensionality reduction \n",
    "    \n",
    "    ('classifier', LogisticRegression())     # or any other classifier deemed useful!\n",
    "\n",
    "   \n",
    "])\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each zoopla csv row for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, zooplatext):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.'),\n",
    "                 'caravanindex':find_str(text,\"caravan\"),\n",
    "                 'parkhomeindex':find_str(text,\"park home\"),\n",
    "                 'bedroomparkhomeindex':find_str(text,\"bedroom park home\"),\n",
    "                 'apartmentindex':find_str(text,\"apartment\"),\n",
    "                 'flatindex':find_str(text,\"flat\"),\n",
    "                 'holidayindex':find_str(text,\"holiday\"),\n",
    "                 'challetindex':find_str(text,\"challet\"),\n",
    "                 'statichomeindex':find_str(text,\"static home\"),\n",
    "                 'staticcaravanindex':find_str(text,\"static caravan\"),\n",
    "                 'mobilehomeindex':find_str(text,\"mobile home\"),\n",
    "                 'parklodgeindex':find_str(text,\"park lodge\"),\n",
    "                 'holidayindex':find_str(text,\"holiday\"),\n",
    "                 'holidayvillageindex':find_str(text,\"holiday village\")\n",
    "                }\n",
    "                for text in zooplatext]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "textstats = Pipeline([\n",
    "             \n",
    "                ('handpickedfeatures', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                ('classifier', LogisticRegression())     # or any other classifier deemed useful!\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samplz=pd.read_csv(PATH_TO_DATA, encoding='latin1', nrows=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time() \n",
    "\n",
    "#load pickled classifier\n",
    "FeatUnion_clf = joblib.load('FeatUnion_clf.pkl') \n",
    "\n",
    "#counter\n",
    "progress=0\n",
    "\n",
    "\n",
    "for chunk in pd.read_csv(PATH_TO_DATA ,chunksize=5000,encoding='latin1'):\n",
    "    \n",
    "    #increment counter\n",
    "    progress = progress + 1\n",
    "    \n",
    "    # print progress and time since started\n",
    "    print(\"chunk # \",progress,\" minutes past since starting: \",(time.time() - start)//60)\n",
    "   \n",
    "    #get the prediction for this chunck of text data\n",
    "\n",
    "    %time Y_pred = FeatUnion_clf.predict(chunk['description'].values.astype('U'))\n",
    "    \n",
    "    #get the probability of prediction for this chunck of text data\n",
    "    \n",
    "    \n",
    "    %time Y_pred_prob = FeatUnion_clf.predict_proba(chunk['description'].values.astype('U'))[:,1]\n",
    "\n",
    "    #create a dataframe to hold the text/prediction/prediction prb / longlat / uprn\n",
    "    \n",
    "    Pred_output_dataframe = pd.DataFrame({'udbc_X':chunk['description'],\n",
    "                                          'Y_pred':Y_pred,\n",
    "                                          'Y_pred_prob':Y_pred_prob,\n",
    "                                          'unique_id' :chunk['unique_id'],\n",
    "                                          'lat':chunk['LATITUDE'], \n",
    "                                          'long':chunk['LONGITUDE'], \n",
    "                                          'uprn' :chunk['UPRN'] })\n",
    "\n",
    "    \n",
    "    # append the dataframe created at each chunk on final csv file\n",
    "    \n",
    "    Pred_output_dataframe.to_csv('finalPredictedCaravansUBDC.csv',\n",
    "                                 index=False,\n",
    "                                 header=False,\n",
    "                                 mode='a',#append data to csv file\n",
    "                                 chunksize=5000)#size of data to append for each loop\n",
    "                                \n",
    "                                \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

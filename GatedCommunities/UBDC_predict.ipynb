{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm, grid_search\n",
    "import time\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ubdc = pd.read_csv('data', encoding='latin1', nrows=20)\n",
    "print(list(ubdc))\n",
    "print(ubdc.dtypes)\n",
    "'''ubdc.to_csv('data')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wf= pd.read_csv('data', encoding='latin1', usecols=['udprn', 'id'])\n",
    "print(list(wf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(wf.id.head())\n",
    "print(len(wf))\n",
    "print(ubdc.listing_id.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq__value_list = wf.id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksize = 10**5\n",
    "chunks = 0\n",
    "start = time.time() \n",
    "start2 = time.time() \n",
    "yes = 0\n",
    "no = 0\n",
    "UBDC = \"data\"\n",
    "cols = ['cols']\n",
    "# Ensures that only the header for the first chunk is kept (as all the files will be appended)\n",
    "writeHeader = True\n",
    "# START: For each chunk in zoopla data:        \n",
    "for chunk in pd.read_csv(UBDC, encoding='latin1',chunksize=chunksize, usecols=cols):\n",
    "    #increment count\n",
    "    chunks +=1\n",
    "    \n",
    "    start_chunk = time.time()\n",
    "    \n",
    "    df_yes = chunk[chunk['listing_id'].isin(uniq__value_list)]\n",
    "    yes+= len(df_yes)\n",
    "    df_no = chunk[~chunk['listing_id'].isin(uniq__value_list)]\n",
    "    no+= len(df_no)\n",
    "    \n",
    "    # Print info after each chunk to keep up to date with progress\n",
    "    end = time.time() - start_chunk\n",
    "    interim = (time.time() - start2)//60\n",
    "    print(\"\\n ****** \\n This iteration :\\t {} \\n Time taken (Seconds):   \\t {} \\n Chunk number (out of 228): \\t {}\\n Interim time (Minutes): \\t {} \\n Total df_yes: \\t {} \\n Total df_no: \\t {} \\n   ****** \\n\" .format(' ',end, chunks, interim, yes, no))\n",
    "    \n",
    "    # 3. Write to csv and tell whether or not to include the header\n",
    "    if writeHeader is True:    \n",
    "        df_no.to_csv('filepath', mode='a', header=True, index=False)\n",
    "        df_yes.to_csv('filepath', mode='a', header=True, index=False)\n",
    "        writeHeader = False\n",
    "    else:\n",
    "        df_no.to_csv('filepath', mode='a', header=False, index=False)\n",
    "        df_yes.to_csv('filepath', mode='a', header=True, index=False) \n",
    "\n",
    "print(\"\\n ****** \\n Chunks Processed: \\t {}\\n Time elapsed (Minutes):  \\t{}\\n ****** \\n\".format(chunks, (time.time() - start)//60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''with open('wf_label.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    preds = pickle.load(f)\n",
    "'''\n",
    "\n",
    "with open('clf.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    clf = pickle.load(f)\n",
    "with open('trigram_LR.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    trigram_LR = pickle.load(f)\n",
    "\n",
    "\n",
    "'''    \n",
    "with open('train_X.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    train_X = pickle.load(f)\n",
    "with open('train_y.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    train_y = pickle.load(f)\n",
    "'''\n",
    "\n",
    "with open('X.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    X_train = pickle.load(f)\n",
    "with open('y.pickle', 'rb') as f:\n",
    "    # The protocol version used isdetected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    y_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to get text in right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopWords(input):\n",
    "    exclude = set(string.punctuation)\n",
    "    output = ' '.join([word for word in input.split() if word not in stopwords.words(\"english\")])\n",
    "    output = ''.join(ch for ch in output if ch not in exclude)\n",
    "        \n",
    "    return pd.Series(dict(output=output))\n",
    "\n",
    "def remove_non_ascii (text):\n",
    "    return ''.join(i for i in text if ord(i)<128)\n",
    "\n",
    "# Stem and lemmatize\n",
    "porter_stemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksize = 10**5\n",
    "chunks = 0\n",
    "start = time.time() \n",
    "start2 = time.time() \n",
    "ubdc = 'data'\n",
    "cols = ['cols']\n",
    "# Ensures that only the header for the first chunk is kept (as all the files will be appended)\n",
    "writeHeader = True\n",
    "# START: For each chunk in zoopla data:        \n",
    "for chunk in pd.read_csv(ubdc, chunksize=chunksize, usecols=cols):\n",
    "    #increment count\n",
    "    chunks +=1\n",
    "    \n",
    "    start_chunk = time.time()\n",
    "    \n",
    "    # 1.  SOME CLEANING OF DESCRIPTION    \n",
    "    chunk['description'] = chunk['description'].astype(str)\n",
    "    chunk[\"description\"] = chunk[\"description\"].str.lower()   \n",
    "    ##  Shouldn't really use regex to parse html, should use  parser, but this was done for quickness\n",
    "    chunk['description'] = chunk['description'].str.replace('\\n', ' ') # this one works\n",
    "    chunk['description'] = chunk['description'].str.replace('&amp;pound;', 'Â£')\n",
    "    chunk['description'] = chunk['description'].str.replace('&amp;', '&')\n",
    "    chunk['description'] = chunk['description'].str.replace('&amp;amp;', '&')\n",
    "    chunk['description'] = chunk['description'].str.replace('&amp;quot;', '\"') \n",
    "    chunk['description'] = chunk['description'].str.replace('&amp;nbsp;', ' ')\n",
    "    chunk['description'] = chunk['description'].str.replace('&amp;#39;', \"'\")\n",
    "    chunk['description'] = chunk['description'].str.replace('&#39;', \"'\")\n",
    "    chunk['description'] = chunk['description'].str.replace('&#x2019;', \"'\")\n",
    "    chunk['description'] = chunk['description'].str.replace('&nbsp;', \"\")\n",
    "    chunk['description'] = chunk['description'].str.replace('&#231;', \"c\")\n",
    "    chunk['description'] = chunk['description'].str.replace('&#x2018', \"'\")\n",
    "    chunk['description'] = chunk['description'].str.replace('&#8217;', \"'\")\n",
    "    chunk['description'] = chunk['description'].str.replace('&quot;', '\"')\n",
    "    chunk['description'] = chunk['description'].str.replace('&#x2013;', '') # meant to be a dash\n",
    "    chunk['description'] = chunk['description'].str.replace('.&#13;&#10;', \" \")\n",
    "    # Apply the function to remove special characters\n",
    "    chunk['description'] = chunk['description'].apply(lambda x : remove_non_ascii(x))\n",
    "    \n",
    "    print('Next step')\n",
    "    # 2. Start removing stop words, lematizing etc\n",
    "    # 2a. Stop words\n",
    "    print('stopwords')\n",
    "    chunk['ml_text'] = chunk['description'].apply(lambda x: removeStopWords(x))\n",
    "    # 2b. Lematizing etc\n",
    "    print('lemma')\n",
    "    chunk['token_text'] = chunk['ml_text'].apply(lambda x : x.split(\" \"))\n",
    "    chunk['stem_text'] = chunk['token_text'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n",
    "    chunk['stem_text'] = chunk['stem_text'].apply(lambda x : ' '.join(x))\n",
    "    # Drop columns no longer needed\n",
    "    chunk.drop(['description', 'ml_text', 'token_text'], axis=1, inplace=True) \n",
    "    \n",
    "    \n",
    "    # Print info after each chunk to keep up to date with progress\n",
    "    end = time.time() - start_chunk\n",
    "    interim = (time.time() - start2)//60\n",
    "    print(\"\\n ****** \\n This iteration :\\t {} \\n Time taken (Seconds):   \\t {} \\n Chunk number (out of 228): \\t {}\\n Interim time (Minutes): \\t {} \\n  ****** \\n\" .format(' ',end, chunks, interim))\n",
    "    \n",
    "    # 3. Write to csv and tell whether or not to include the header\n",
    "    if writeHeader is True:    \n",
    "        chunk.to_csv('filepath', mode='a', header=True, index=False)\n",
    "        writeHeader = False\n",
    "    else:\n",
    "        chunk.to_csv('filepath', mode='a', header=False, index=False)\n",
    "\n",
    "print(\"\\n ****** \\n Chunks Processed: \\t {}\\n Time elapsed (Minutes):  \\t{}\\n ****** \\n\".format(chunks, (time.time() - start)//60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksize = 10**5\n",
    "chunks = 0\n",
    "start = time.time() \n",
    "start2 = time.time() \n",
    "ubdc = 'data'\n",
    "cols = ['cols']\n",
    "# Ensures that only the header for the first chunk is kept (as all the files will be appended)\n",
    "writeHeader = True\n",
    "# START: For each chunk in zoopla data:        \n",
    "for chunk in pd.read_csv(ubdc, chunksize=chunksize, usecols=cols):\n",
    "    #increment count\n",
    "    chunks +=1\n",
    "    \n",
    "    start_chunk = time.time()\n",
    "    \n",
    "    X=chunk['stem_text'].values.astype('U')\n",
    "    trigram_LR.fit(X_train, y_train)\n",
    "    predictions= trigram_LR.predict(X)\n",
    "    y_pred_prob = trigram_LR.predict_proba(X)[:,1]\n",
    "    X_LR_output = pd.DataFrame({'X' :, 'predictions':predictions, 'pred_prob' :y_pred_prob, 'unique_id' :chunk['unique_id'],'lat':chunk['LATITUDE'], 'long':chunk['LONGITUDE']})\n",
    "    #LR_output = pd.DataFrame({'X':X, 'predicted':y_pred_prob, 'listing_id':df_A['listing_id'], 'lat':df_A['lat'], 'long':df_A['long'] })\n",
    "    sout = X_LR_output.sort_values(['X', 'predictions'], ascending=[False, False])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Print info after each chunk to keep up to date with progress\n",
    "    end = time.time() - start_chunk\n",
    "    interim = (time.time() - start2)//60\n",
    "    print(\"\\n ****** \\n This iteration :\\t {} \\n Time taken (Seconds):   \\t {} \\n Chunk number (out of 228): \\t {}\\n Interim time (Minutes): \\t {} \\n  ****** \\n\" .format(' ',end, chunks, interim))\n",
    "    \n",
    "    # 3. Write to csv and tell whether or not to include the header\n",
    "    if writeHeader is True:    \n",
    "        sout.to_csv('filepath', mode='a', header=True, index=False)\n",
    "        writeHeader = False\n",
    "    else:\n",
    "        sout.to_csv('filepath', mode='a', header=False, index=False)\n",
    "\n",
    "print(\"\\n ****** \\n Chunks Processed: \\t {}\\n Time elapsed (Minutes):  \\t{}\\n ****** \\n\".format(chunks, (time.time() - start)//60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/kim/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm, grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_A = pd.read_csv('data_A.csv', encoding='latin1')\n",
    "print(df_A.shape)\n",
    "df_A.dtypes\n",
    "df_A['description'] = df_A['description'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('wf_label.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    preds = pickle.load(f)\n",
    "with open('clf.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    clf = pickle.load(f)\n",
    "with open('trigram_LR.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    trigram_LR = pickle.load(f)\n",
    "\n",
    "\n",
    "    \n",
    "with open('train_X.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    train_X = pickle.load(f)\n",
    "with open('train_y.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    train_y = pickle.load(f)\n",
    "    \n",
    "with open('X.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    X_train = pickle.load(f)\n",
    "with open('y.pickle', 'rb') as f:\n",
    "    # The protocol version used isdetected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    y_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'property_id', 'num_bathrooms', 'listing_status', 'listing_id', 'num_bedrooms', 'num_recepts', 'last_marketed_date', 'description', 'property_type', 'price', 'num_floors', 'first_marketed_date', 'property_number', 'postcode', 'address', 'oseast1m', 'osnrth1m', 'laua', 'msoa11', 'lat', 'long']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_A.drop(['Unnamed: 0'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_A.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the id from the labelled dataset it in the WF data, remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "      ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n"
     ]
    }
   ],
   "source": [
    "print(trigram_LR.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create predictions\n",
    "# 1. get text in right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def removeStopWords(input):\n",
    "    exclude = set(string.punctuation)\n",
    "    output = ' '.join([word for word in input.split() if word not in stopwords.words(\"english\")])\n",
    "    output = ''.join(ch for ch in output if ch not in exclude)\n",
    "        \n",
    "    return pd.Series(dict(output=output))\n",
    "\n",
    "df_A['ml_text'] = df_A['description'].apply(lambda x: removeStopWords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_A['ml_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii (text):\n",
    "    return ''.join(i for i in text if ord(i)<128)\n",
    "                   \n",
    "def replacenon_ascii_w_space (text):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "#df['test'] = df['ml_text'].apply(lambda x : x.split(\" \"))\n",
    "#df['test'].apply(filter(lambda x : ' '.join([nonAscii(y) for y in x]))\n",
    "## No success. \n",
    "\n",
    "df_A['ml_text'] = df_A['ml_text'].apply(lambda x: remove_non_ascii(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem and lemmatize\n",
    "porter_stemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_A['token_text'] = df_A['ml_text'].apply(lambda x : x.split(\" \"))\n",
    "df_A['stem_text'] = df_A['token_text'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n",
    "df_A['stem_text'] = df_A['stem_text'].apply(lambda x : ' '.join(x))\n",
    "# unseen_wf.drop(['Unnamed: 0', 'description_parsed', 'text', 'token_text', 'ml_text'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('df_A.pickle', 'wb') as f:\n",
    "    pickle.dump(df_A,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=df_A['stem_text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1, X2, X3, X4, X5 = np.split(df_A['stem_text'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open('X1.pickle', 'wb') as f:\n",
    "    pickle.dump(X1,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_prob = trigram_LR.predict_proba(X)[:,1]\n",
    "LR_output = pd.DataFrame({'X':X, 'predicted':y_pred_prob, 'listing_id':df_A['listing_id'], 'lat':df_A['lat'], 'long':df_A['long'] })\n",
    "sout = LR_output.sort_values(['X', 'predicted'], ascending=[False, False])\n",
    "sout.to_html(\"LRCLF_A_test.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_LR.fit(X_train, y_train)\n",
    "predictions_1 = trigram_LR.predict(X1)\n",
    "predictions_2 = trigram_LR.predict(X2)\n",
    "predictions_3 = trigram_LR.predict(X3)\n",
    "predictions_4 = trigram_LR.predict(X4)\n",
    "predictions_5 = trigram_LR.predict(X5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_prob_1= trigram_LR.predict_proba(X1)[:,1]\n",
    "pred_prob_2= trigram_LR.predict_proba(X2)[:,1]\n",
    "pred_prob_3= trigram_LR.predict_proba(X3)[:,1]\n",
    "pred_prob_4= trigram_LR.predict_proba(X4)[:,1]\n",
    "pred_prob_5= trigram_LR.predict_proba(X5)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open('predictions_1.pickle', 'wb') as f:\n",
    "    pickle.dump(predictions_1,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('predictions_2.pickle', 'wb') as f:\n",
    "    pickle.dump(predictions_2,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('predictions_3.pickle', 'wb') as f:\n",
    "    pickle.dump(predictions_3,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open('pred_prob_1.pickle', 'wb') as f:\n",
    "    pickle.dump(pred_prob_1,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('pred_prob_2.pickle', 'wb') as f:\n",
    "    pickle.dump(pred_prob_2,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('pred_prob_3.pickle', 'wb') as f:\n",
    "    pickle.dump(pred_prob_3,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "id_A= df_A['listing_id'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#udprn = unseen_wf['udprn'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to concat the differenct X and predictions arrays and put into one array for each\n",
    "X_con = np.concatenate((X1,X2,X3, X4, X5),axis=0)\n",
    "predictions = np.concatenate((predictions_1, predictions_2, predictions_3, predictions_4, predictions_5),axis=0)\n",
    "pred_prob = np.concatenate((pred_prob_1, pred_prob_2, pred_prob_3,pred_prob_4, pred_prob_5),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_LR_output = pd.DataFrame({'X' :X_con, 'predictions':predictions, 'pred_prob' :pred_prob, 'listing_id' :df_A['listing_id'],'lat':df_A['lat'], 'long':df_A['long']})\n",
    "sout = X_LR_output.sort_values(['pred_prob', 'predictions'], ascending=[False, False])\n",
    "sout.to_csv(\"LR_clf_pred_A.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('LR_predictions_A.pickle', 'wb') as f:\n",
    "    pickle.dump(X_LR_output,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_LR_output.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gate = X_LR_output[X_LR_output['predictions']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('gate_A_pred.pickle', 'wb') as f:\n",
    "    pickle.dump(gate,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open('gate_A_pred.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    gate = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gate.to_csv('gate_pred_A.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "count_vect = TfidfVectorizer()\n",
    "X_transformed = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit_transform(X_transformed)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print(centroids)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = [\"g.\",\"r.\",\"c.\",\"y.\"]\n",
    "\n",
    "for i in range(len(X_transformed)):\n",
    "    print(\"coordinate:\",X_transformed[i], \"label:\", labels[i])\n",
    "    plt.plot(X_transformed[i][0], X_transformed[i][1], colors[labels[i]], markersize = 10)\n",
    "\n",
    "\n",
    "plt.scatter(centroids[:, 0],centroids[:, 1], marker = \"x\", s=150, linewidths = 5, zorder = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating predictions for identifying restricted access properties in Zoopla data\n",
    "### What are restricted access properties?\n",
    "- Restricted access properties are properties such as secure access flats or gated communities\n",
    "- These are either inconsistently recorded in other data sources or not at all\n",
    "- Identifying them will improve the Address Register\n",
    "- It will also help to make field work more efficient if enumerators know they will have difficulty with access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import packages/ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm, grid_search\n",
    "import time\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we open 'pickles' for the classifier we built earlier and our training data for X and y\n",
    "- The pickle basically preserves data/ objects in your code to be used elsewhere- handy so that we do not have to re-create the classifier etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('trigram_LR.pickle', 'rb') as f:\n",
    "    # This opens the pickled version of our chosen classifier\n",
    "    trigram_LR = pickle.load(f)\n",
    "with open('X.pickle', 'rb') as f:\n",
    "    # This provides our training data for X\n",
    "    X_train = pickle.load(f)\n",
    "with open('y.pickle', 'rb') as f:\n",
    "    # Here we import the training data for y\n",
    "    y_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import sets of Zoopla data. We used WhenFresh data for training so we want to remove these records from the Urban Big Data Centre data we are using for predictions. Therefore we'll import both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wf= pd.read_csv('whenfresh_data_linked.csv', encoding='latin1', usecols=['udprn', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating a list of the unique ids in Whenfresh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq__value_list = wf.id.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use this list to remove any unique ids shared between UBDC and WF. We'll output to a file as this can be used in the caravan work strand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksize = 10**5\n",
    "chunks = 0\n",
    "start = time.time() \n",
    "start2 = time.time() \n",
    "yes = 0\n",
    "no = 0\n",
    "UBDC = \"UBDC.csv\"\n",
    "cols = ['description', 'listing_id','unique_id','UPRN', 'LATITUDE', 'LONGITUDE']\n",
    "# Ensures that only the header for the first chunk is kept (as all the files will be appended)\n",
    "writeHeader = True\n",
    "# START: For each chunk in zoopla data:        \n",
    "for chunk in pd.read_csv(UBDC, encoding='latin1',chunksize=chunksize, usecols=cols):\n",
    "    #increment count\n",
    "    chunks +=1\n",
    "    start_chunk = time.time()\n",
    "    \n",
    "    df_yes = chunk[chunk['listing_id'].isin(uniq__value_list)]\n",
    "    df_no = chunk[~chunk['listing_id'].isin(uniq__value_list)] \n",
    "    # Print info after each chunk to keep up to date with progress\n",
    "    end = time.time() - start_chunk\n",
    "    \n",
    "    # 3. Write to csv and tell whether or not to include the header\n",
    "    if writeHeader is True:    \n",
    "        df_no.to_csv('UBDC_not_wf.csv', mode='a', header=True, index=False)\n",
    "        df_yes.to_csv('UBDC_and_wf.csv', mode='a', header=True, index=False)\n",
    "        writeHeader = False\n",
    "    else:\n",
    "        df_no.to_csv('UBDC_not_wf.csv', mode='a', header=False, index=False)\n",
    "        df_yes.to_csv('UBDC_and_wf.csv', mode='a', header=True, index=False) \n",
    "\n",
    "print(\"\\n ****** \\n Chunks Processed: \\t {}\\n Time elapsed (Minutes):  \\t{}\\n ****** \\n\".format(chunks, (time.time() - start)//60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to get text in right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define stemmer \n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# 20 x  faster  for this little code here\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def removeStopWords(input):\n",
    "    exclude = set(string.punctuation)\n",
    "    output = ' '.join([word for word in input.split() if word not in cachedStopWords])\n",
    "    output = ''.join(ch for ch in output if ch not in exclude)\n",
    "        \n",
    "    return pd.Series(dict(output=output))\n",
    "\n",
    "  \n",
    "def remove_non_ascii (text):\n",
    "    return ''.join(i for i in text if ord(i)<128)\n",
    "\n",
    "def remove_non_ascii_df (text):\n",
    "    return pd.Series(dict(output = ''.join(i for i in text if ord(i)<128)))\n",
    "\n",
    "def stripHTML(input):\n",
    "    output = BeautifulSoup(input, \"lxml\").text\n",
    "    return pd.Series(dict(output = output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform machine learning predictions using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we fit our model\n",
    "trigram_LR.fit(X_train, y_train)\n",
    "\n",
    "# Define sizes of chunks, iterate over the data to make predictions, structure data and output to csv \n",
    "chunksize = 5000\n",
    "chunks = 0\n",
    "start = time.time() \n",
    "ubdc = 'UBDC_not_wf.csv'\n",
    "cols = ['description', 'unique_id','UPRN', 'LATITUDE', 'LONGITUDE']\n",
    "# Ensures that only the header for the first chunk is kept (as all the files will be appended)\n",
    "writeHeader = True\n",
    "# START: For each chunk in zoopla data:        \n",
    "for chunk in pd.read_csv(ubdc, chunksize=chunksize, usecols=cols):\n",
    "    #increment count\n",
    "    chunks +=1\n",
    "    \n",
    "    print(\"chunk # \",chunks,\" minutes past since starting: \",(time.time() - start)//60)\n",
    "    %time chunk['description']  = chunk['description'].astype(str).apply(lambda x : remove_non_ascii(x))\n",
    "    %time chunk['description']  = chunk['description'].apply(lambda x : removeStopWords(x))\n",
    "    %time chunk['description']  = chunk['description'].apply(stripHTML)\n",
    "    %time chunk['description']  = chunk['description'].apply(lambda x: ' '.join([porter_stemmer.stem(y) for y in x.split()]))\n",
    "    \n",
    "    print(\"Start predictions\")\n",
    "    %time X=chunk['description'].values.astype('U')\n",
    "    %time predictions= trigram_LR.predict(X)\n",
    "    %time y_pred_prob = trigram_LR.predict_proba(X)[:,1]\n",
    "    %time X_LR_output = pd.DataFrame({'X' : X, 'predictions':predictions, 'pred_prob' :y_pred_prob, 'unique_id' :chunk['unique_id'],'lat':chunk['LATITUDE'], 'long':chunk['LONGITUDE']})     \n",
    "    \n",
    "    # 3. Write to csv and tell whether or not to include the header\n",
    "    if writeHeader is True:    \n",
    "        X_LR_output.to_csv('UBDC_predictions.csv', mode='a', header=True, index=False)\n",
    "        writeHeader = False\n",
    "    else:\n",
    "        X_LR_output.to_csv('UBDC_predictions.csv', mode='a', header=False, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
